{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "trainfile = \"C:/Users/bahubali/Documents/stanford140/train.csv\"\n",
    "train = pd.read_csv(trainfile,header = None,encoding = 'iso-8859-1')\n",
    "\n",
    "testfile = \"C:/Users/bahubali/Documents/stanford140/test.csv\"\n",
    "test = pd.read_csv(testfile,header = None,encoding = 'iso-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      "0    1600000 non-null int64\n",
      "1    1600000 non-null int64\n",
      "2    1600000 non-null object\n",
      "3    1600000 non-null object\n",
      "4    1600000 non-null object\n",
      "5    1600000 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.600000e+06</td>\n",
       "      <td>1.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.998818e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000001e+00</td>\n",
       "      <td>1.935761e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.467810e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.956916e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.002102e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.177059e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.329206e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1\n",
       "count  1.600000e+06  1.600000e+06\n",
       "mean   2.000000e+00  1.998818e+09\n",
       "std    2.000001e+00  1.935761e+08\n",
       "min    0.000000e+00  1.467810e+09\n",
       "25%    0.000000e+00  1.956916e+09\n",
       "50%    2.000000e+00  2.002102e+09\n",
       "75%    4.000000e+00  2.177059e+09\n",
       "max    4.000000e+00  2.329206e+09"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 498 entries, 0 to 497\n",
      "Data columns (total 6 columns):\n",
      "0    498 non-null int64\n",
      "1    498 non-null int64\n",
      "2    498 non-null object\n",
      "3    498 non-null object\n",
      "4    498 non-null object\n",
      "5    498 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 23.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re #import regular expressions\n",
    "from bs4 import BeautifulSoup # Import BeautifulSoup into your workspace\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bahubali\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b' i just received my G8 viola exam.. and its... well... .. disappointing.. :\\\\..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Users\\bahubali\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'E3 ON PLAYSTATION HOME IN ABOUT AN HOUR!!!!!!!!!! \\\\../  \\\\../'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "def tweet_to_words(raw_tweet):\n",
    "    # Function to convert a raw tweet to a string of words\n",
    "    # The input is a single string (a raw tweet), and \n",
    "    # the output is a single string (a preprocessed tweet)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    tweet_text = BeautifulSoup(raw_tweet,\"lxml\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", tweet_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) \n",
    "\n",
    "bagofwords = [tweet_to_words(tweet) for tweet in train[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switchfoot http twitpic com zl awww bummer shoulda got david carr third day\n"
     ]
    }
   ],
   "source": [
    "print(bagofwords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bagofwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 200) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(bagofwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 200)\n"
     ]
    }
   ],
   "source": [
    "train_data_features = train_data_features.toarray()\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actually', 'already', 'also', 'always', 'amazing', 'another', 'away', 'awesome', 'baby', 'back', 'bad', 'bed', 'best', 'better', 'big', 'birthday', 'bit', 'bored', 'cant', 'com', 'come', 'coming', 'cool', 'could', 'damn', 'day', 'days', 'done', 'dont', 'early', 'even', 'ever', 'everyone', 'excited', 'feel', 'feeling', 'finally', 'find', 'first', 'follow', 'friend', 'friends', 'fun', 'game', 'get', 'getting', 'girl', 'glad', 'go', 'god', 'going', 'gonna', 'good', 'got', 'gotta', 'great', 'guess', 'guys', 'haha', 'happy', 'hard', 'hate', 'hear', 'help', 'hey', 'hi', 'home', 'hope', 'hot', 'hours', 'house', 'http', 'im', 'keep', 'know', 'last', 'late', 'later', 'left', 'let', 'life', 'like', 'little', 'live', 'lol', 'long', 'look', 'looking', 'looks', 'lost', 'lot', 'love', 'ly', 'made', 'make', 'makes', 'man', 'maybe', 'might', 'miss', 'missed', 'mom', 'morning', 'movie', 'much', 'music', 'need', 'never', 'new', 'next', 'nice', 'night', 'nothing', 'oh', 'ok', 'old', 'omg', 'one', 'party', 'people', 'phone', 'play', 'please', 'pretty', 'rain', 'ready', 'really', 'right', 'sad', 'said', 'saw', 'say', 'school', 'see', 'show', 'sick', 'since', 'sleep', 'someone', 'something', 'song', 'soon', 'sorry', 'start', 'still', 'stuff', 'sucks', 'summer', 'sun', 'sure', 'take', 'tell', 'th', 'thank', 'thanks', 'thats', 'thing', 'things', 'think', 'though', 'thought', 'time', 'tired', 'today', 'tomorrow', 'tonight', 'try', 'trying', 'tweet', 'twitpic', 'twitter', 'two', 'ugh', 'ur', 'us', 'wait', 'waiting', 'wanna', 'want', 'watch', 'watching', 'way', 'weather', 'week', 'weekend', 'well', 'went', 'wish', 'work', 'working', 'world', 'would', 'wow', 'ya', 'yay', 'yeah', 'year', 'yes', 'yesterday', 'yet']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9997 actually\n",
      "14779 already\n",
      "10442 also\n",
      "14947 always\n",
      "11851 amazing\n",
      "14577 another\n",
      "12299 away\n",
      "18331 awesome\n",
      "10564 baby\n",
      "56986 back\n",
      "27403 bad\n",
      "22263 bed\n",
      "16495 best\n",
      "23250 better\n",
      "11312 big\n",
      "10406 birthday\n",
      "24433 bit\n",
      "10647 bored\n",
      "17684 cant\n",
      "52760 com\n",
      "25135 come\n",
      "10396 coming\n",
      "14547 cool\n",
      "21925 could\n",
      "11733 damn\n",
      "89857 day\n",
      "19550 days\n",
      "15338 done\n",
      "18238 dont\n",
      "11826 early\n",
      "19201 even\n",
      "13155 ever\n",
      "16405 everyone\n",
      "9818 excited\n",
      "30274 feel\n",
      "15155 feeling\n",
      "11945 finally\n",
      "14196 find\n",
      "16813 first\n",
      "11717 follow\n",
      "11385 friend\n",
      "14604 friends\n",
      "28716 fun\n",
      "9709 game\n",
      "82202 get\n",
      "24368 getting\n",
      "11121 girl\n",
      "10535 glad\n",
      "74135 go\n",
      "9558 god\n",
      "64635 going\n",
      "23831 gonna\n",
      "91389 good\n",
      "61466 got\n",
      "9298 gotta\n",
      "33539 great\n",
      "11510 guess\n",
      "14085 guys\n",
      "31518 haha\n",
      "27084 happy\n",
      "9729 hard\n",
      "19833 hate\n",
      "10049 hear\n",
      "11835 help\n",
      "19145 hey\n",
      "9532 hi\n",
      "40557 home\n",
      "33922 hope\n",
      "11120 hot\n",
      "13384 hours\n",
      "13085 house\n",
      "71582 http\n",
      "52377 im\n",
      "11299 keep\n",
      "52066 know\n",
      "35765 last\n",
      "9245 late\n",
      "10211 later\n",
      "11497 left\n",
      "15056 let\n",
      "16681 life\n",
      "78595 like\n",
      "17047 little\n",
      "10591 live\n",
      "59286 lol\n",
      "17168 long\n",
      "13393 look\n",
      "13543 looking\n",
      "10438 looks\n",
      "11497 lost\n",
      "9223 lot\n",
      "65002 love\n",
      "14030 ly\n",
      "13843 made\n",
      "24987 make\n",
      "9818 makes\n",
      "14170 man\n",
      "12498 maybe\n",
      "9767 might\n",
      "37152 miss\n",
      "10955 missed\n",
      "9201 mom\n",
      "34610 morning\n",
      "12967 movie\n",
      "37064 much\n",
      "9177 music\n",
      "35994 need\n",
      "17955 never\n",
      "42496 new\n",
      "18678 next\n",
      "23595 nice\n",
      "43984 night\n",
      "10990 nothing\n",
      "40068 oh\n",
      "16257 ok\n",
      "11984 old\n",
      "12264 omg\n",
      "54204 one\n",
      "9924 party\n",
      "20637 people\n",
      "13505 phone\n",
      "9700 play\n",
      "16574 please\n",
      "12706 pretty\n",
      "10952 rain\n",
      "13980 ready\n",
      "50042 really\n",
      "27891 right\n",
      "29430 sad\n",
      "9613 said\n",
      "9911 saw\n",
      "16535 say\n",
      "20618 school\n",
      "46397 see\n",
      "16374 show\n",
      "16044 sick\n",
      "9449 since\n",
      "28009 sleep\n",
      "11522 someone\n",
      "14338 something\n",
      "10096 song\n",
      "17850 soon\n",
      "26383 sorry\n",
      "10530 start\n",
      "43571 still\n",
      "9938 stuff\n",
      "10890 sucks\n",
      "12085 summer\n",
      "10258 sun\n",
      "14918 sure\n",
      "17126 take\n",
      "10417 tell\n",
      "10122 th\n",
      "18120 thank\n",
      "40339 thanks\n",
      "9540 thats\n",
      "14898 thing\n",
      "11633 things\n",
      "41425 think\n",
      "24013 though\n",
      "11167 thought\n",
      "57995 time\n",
      "16408 tired\n",
      "68230 today\n",
      "34131 tomorrow\n",
      "26012 tonight\n",
      "10525 try\n",
      "12407 trying\n",
      "12779 tweet\n",
      "20830 twitpic\n",
      "33423 twitter\n",
      "10322 two\n",
      "10062 ugh\n",
      "13124 ur\n",
      "14959 us\n",
      "22274 wait\n",
      "9791 waiting\n",
      "16586 wanna\n",
      "42308 want\n",
      "15822 watch\n",
      "22779 watching\n",
      "24501 way\n",
      "10282 weather\n",
      "21766 week\n",
      "18506 weekend\n",
      "42791 well\n",
      "13306 went\n",
      "28303 wish\n",
      "65053 work\n",
      "16864 working\n",
      "9491 world\n",
      "27332 would\n",
      "11163 wow\n",
      "10964 ya\n",
      "13711 yay\n",
      "22472 yeah\n",
      "11152 year\n",
      "18706 yes\n",
      "9553 yesterday\n",
      "13401 yet\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the random forest...\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, train[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n",
      "stellargirl loooooooovvvvvveee kindle dx cool fantastic right\n"
     ]
    }
   ],
   "source": [
    "test_bagofwords = [tweet_to_words(tweet) for tweet in test[5]]\n",
    "print(len(test_bagofwords))\n",
    "print(test_bagofwords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(test_bagofwords)\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n",
      "[4 4 4 4 0 4 0 4 4 4 0 0 0 4 4 4 0 4 0 4 0 4 0 4 0 4 4 0 4 4 4 4 0 0 0 0 4\n",
      " 0 0 0 0 4 0 0 4 4 4 0 4 0 0 4 4 4 4 4 4 4 4 0 0 4 4 4 4 4 4 4 4 4 4 4 0 0\n",
      " 4 4 4 4 0 0 4 0 4 4 4 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 4 4 4 4 4 0 0 4 4 4 4\n",
      " 4 4 4 4 0 0 4 4 4 0 4 4 0 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4 0 0 0 0 0 4 0\n",
      " 0 0 4 4 0 4 4 4 4 0 4 4 4 4 0 4 0 4 4 0 4 4 0 4 4 4 4 4 0 4 4 4 0 4 4 4 0\n",
      " 4 4 4 4 4 4 0 0 4 0 4 4 4 4 4 4 4 4 0 4 0 0 4 4 4 0 0 4 0 0 0 0 0 0 4 0 4\n",
      " 4 4 0 0 4 0 0 4 0 4 4 4 4 0 4 4 4 4 4 0 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 0 4 4 4 4 4 4 4 0 0 0 4 4 0 4 4 4 0 4 0 4 4 4 0 4 4 4 4 4 0 0 4 0\n",
      " 0 0 4 4 4 4 4 4 4 4 0 0 4 4 4 4 0 4 0 4 0 4 4 4 0 4 4 4 4 4 4 4 4 4 4 4 0\n",
      " 0 0 4 0 4 4 4 0 4 4 4 0 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4\n",
      " 4 4 4 0 4 4 0 0 0 0 4 0 4 0 4 4 4 4 4 0 0 0 4 4 0 0 0 4 4 4 0 4 0 4 4 0 0\n",
      " 4 0 0 4 4 4 4 4 4 4 0 4 4 0 4 0 4 4 4 4 0 0 0 4 4 4 4 4 4 4 4 4 4 0 4 4 0\n",
      " 4 4 4 4 4 4 4 4 4 0 4 0 4 4 0 4 4 4 4 0 0 4 0 4 0 4 0 4 4 4 0 4 4 4 4 0 4\n",
      " 0 0 4 4 4 4 4 4 4 4 0 0 4 0 0 4 0]\n"
     ]
    }
   ],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "print(len(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.491967871486\n",
      "[[103   0  74]\n",
      " [ 17   0 122]\n",
      " [ 40   0 142]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(accuracy_score(test[0],result))\n",
    "print(confusion_matrix(test[0],result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
