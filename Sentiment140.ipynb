{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "trainfile = \"C:/Users/bahubali/Documents/stanford140/train.csv\"\n",
    "train = pd.read_csv(trainfile,header = None,encoding = 'iso-8859-1')\n",
    "\n",
    "testfile = \"C:/Users/bahubali/Documents/stanford140/test.csv\"\n",
    "test = pd.read_csv(testfile,header = None,encoding = 'iso-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      "0    1600000 non-null int64\n",
      "1    1600000 non-null int64\n",
      "2    1600000 non-null object\n",
      "3    1600000 non-null object\n",
      "4    1600000 non-null object\n",
      "5    1600000 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.600000e+06</td>\n",
       "      <td>1.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.998818e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000001e+00</td>\n",
       "      <td>1.935761e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.467810e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.956916e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.002102e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.177059e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.329206e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1\n",
       "count  1.600000e+06  1.600000e+06\n",
       "mean   2.000000e+00  1.998818e+09\n",
       "std    2.000001e+00  1.935761e+08\n",
       "min    0.000000e+00  1.467810e+09\n",
       "25%    0.000000e+00  1.956916e+09\n",
       "50%    2.000000e+00  2.002102e+09\n",
       "75%    4.000000e+00  2.177059e+09\n",
       "max    4.000000e+00  2.329206e+09"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 498 entries, 0 to 497\n",
      "Data columns (total 6 columns):\n",
      "0    498 non-null int64\n",
      "1    498 non-null int64\n",
      "2    498 non-null object\n",
      "3    498 non-null object\n",
      "4    498 non-null object\n",
      "5    498 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 23.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nltk version is 3.2.2.\n"
     ]
    }
   ],
   "source": [
    "print('The nltk version is {}.'.format(nltk.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re #import regular expressions\n",
    "from bs4 import BeautifulSoup # Import BeautifulSoup into your workspace\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bahubali\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b' i just received my G8 viola exam.. and its... well... .. disappointing.. :\\\\..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Users\\bahubali\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'E3 ON PLAYSTATION HOME IN ABOUT AN HOUR!!!!!!!!!! \\\\../  \\\\../'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "tweet_text = [BeautifulSoup(raw_tweet,\"lxml\").get_text() for raw_tweet in train[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "1600000\n"
     ]
    }
   ],
   "source": [
    "print(tweet_text[0])\n",
    "print(len(tweet_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letters_only = [re.sub(\"[^a-zA-Z]\", \" \", tweet) for tweet in tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " switchfoot http   twitpic com  y zl   Awww  that s a bummer   You shoulda got David Carr of Third Day to do it   D\n",
      "1600000\n"
     ]
    }
   ],
   "source": [
    "print(letters_only[0])\n",
    "print(len(letters_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [tweets.lower().split() for tweets in letters_only]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['switchfoot', 'http', 'twitpic', 'com', 'y', 'zl', 'awww', 'that', 's', 'a', 'bummer', 'you', 'shoulda', 'got', 'david', 'carr', 'of', 'third', 'day', 'to', 'do', 'it', 'd']\n",
      "1600000\n"
     ]
    }
   ],
   "source": [
    "print(words[0])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "{'why', 'being', 'have', 'up', 'yours', 'but', 'with', 'all', 'does', 'am', 'a', 'under', 'will', 'these', 'where', 'ain', 'an', 'if', 'doing', 'your', 'other', 'are', 'ma', 'here', 'hers', 'herself', 'both', 'against', 'doesn', 'off', 'before', 'having', 'theirs', 'such', 'been', 'now', 'him', 'as', 'yourselves', 'its', 'above', 'further', 'itself', 'needn', 'about', 'some', 'or', 'he', 'of', 'into', 'our', 'while', 'which', 'more', 'ourselves', 'at', 'down', 'by', 'few', 'and', 're', 'won', 'be', 'didn', 'then', 'aren', 'on', 'the', 'hasn', 'through', 'in', 'very', 'after', 'only', 'so', 'whom', 'is', 'until', 'll', 'mustn', 'wouldn', 't', 'we', 'don', 'most', 've', 'any', 'same', 'over', 'd', 'shan', 'them', 'to', 'himself', 'from', 'when', 'his', 'those', 'o', 'out', 'nor', 'how', 'wasn', 'themselves', 'mightn', 'i', 'own', 'just', 'did', 'shouldn', 'there', 'do', 'were', 'their', 'below', 'too', 'who', 'no', 'what', 's', 'should', 'myself', 'me', 'ours', 'each', 'hadn', 'than', 'once', 'was', 'this', 'during', 'that', 'had', 'she', 'because', 'isn', 'm', 'couldn', 'not', 'haven', 'y', 'my', 'between', 'her', 'they', 'yourself', 'has', 'for', 'weren', 'again', 'you', 'it', 'can'}\n"
     ]
    }
   ],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "print(len(stops))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(tweet_words):\n",
    "    return([w for w in tweet_words if w not in stops])\n",
    "\n",
    "meaningful_words = [f(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['switchfoot', 'http', 'twitpic', 'com', 'zl', 'awww', 'bummer', 'shoulda', 'got', 'david', 'carr', 'third', 'day']\n",
      "1600000\n"
     ]
    }
   ],
   "source": [
    "print(meaningful_words[0])\n",
    "print(len(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bagofwords = []\n",
    "\n",
    "def list_to_string(l):\n",
    "    return(\" \".join(l))\n",
    "\n",
    "\n",
    "bagofwords = [list_to_string(w) for w in meaningful_words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n",
      "switchfoot http twitpic com zl awww bummer shoulda got david carr third day\n"
     ]
    }
   ],
   "source": [
    "print(len(bagofwords))\n",
    "print(bagofwords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 100) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(bagofwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awesome', 'back', 'bad', 'bed', 'best', 'better', 'bit', 'cant', 'com', 'come', 'could', 'day', 'days', 'done', 'dont', 'even', 'everyone', 'feel', 'feeling', 'first', 'fun', 'get', 'getting', 'go', 'going', 'gonna', 'good', 'got', 'great', 'haha', 'happy', 'hate', 'hey', 'home', 'hope', 'http', 'im', 'know', 'last', 'let', 'life', 'like', 'little', 'lol', 'long', 'love', 'make', 'miss', 'morning', 'much', 'need', 'never', 'new', 'next', 'nice', 'night', 'oh', 'ok', 'one', 'people', 'please', 'really', 'right', 'sad', 'say', 'school', 'see', 'show', 'sick', 'sleep', 'soon', 'sorry', 'still', 'take', 'thank', 'thanks', 'think', 'though', 'time', 'tired', 'today', 'tomorrow', 'tonight', 'twitpic', 'twitter', 'wait', 'wanna', 'want', 'watch', 'watching', 'way', 'week', 'weekend', 'well', 'wish', 'work', 'working', 'would', 'yeah', 'yes']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18331 awesome\n",
      "56986 back\n",
      "27403 bad\n",
      "22263 bed\n",
      "16495 best\n",
      "23250 better\n",
      "24433 bit\n",
      "17684 cant\n",
      "52760 com\n",
      "25135 come\n",
      "21925 could\n",
      "89857 day\n",
      "19550 days\n",
      "15338 done\n",
      "18238 dont\n",
      "19201 even\n",
      "16405 everyone\n",
      "30274 feel\n",
      "15155 feeling\n",
      "16813 first\n",
      "28716 fun\n",
      "82202 get\n",
      "24368 getting\n",
      "74135 go\n",
      "64635 going\n",
      "23831 gonna\n",
      "91389 good\n",
      "61466 got\n",
      "33539 great\n",
      "31518 haha\n",
      "27084 happy\n",
      "19833 hate\n",
      "19145 hey\n",
      "40557 home\n",
      "33922 hope\n",
      "71582 http\n",
      "52377 im\n",
      "52066 know\n",
      "35765 last\n",
      "15056 let\n",
      "16681 life\n",
      "78595 like\n",
      "17047 little\n",
      "59286 lol\n",
      "17168 long\n",
      "65002 love\n",
      "24987 make\n",
      "37152 miss\n",
      "34610 morning\n",
      "37064 much\n",
      "35994 need\n",
      "17955 never\n",
      "42496 new\n",
      "18678 next\n",
      "23595 nice\n",
      "43984 night\n",
      "40068 oh\n",
      "16257 ok\n",
      "54204 one\n",
      "20637 people\n",
      "16574 please\n",
      "50042 really\n",
      "27891 right\n",
      "29430 sad\n",
      "16535 say\n",
      "20618 school\n",
      "46397 see\n",
      "16374 show\n",
      "16044 sick\n",
      "28009 sleep\n",
      "17850 soon\n",
      "26383 sorry\n",
      "43571 still\n",
      "17126 take\n",
      "18120 thank\n",
      "40339 thanks\n",
      "41425 think\n",
      "24013 though\n",
      "57995 time\n",
      "16408 tired\n",
      "68230 today\n",
      "34131 tomorrow\n",
      "26012 tonight\n",
      "20830 twitpic\n",
      "33423 twitter\n",
      "22274 wait\n",
      "16586 wanna\n",
      "42308 want\n",
      "15822 watch\n",
      "22779 watching\n",
      "24501 way\n",
      "21766 week\n",
      "18506 weekend\n",
      "42791 well\n",
      "28303 wish\n",
      "65053 work\n",
      "16864 working\n",
      "27332 would\n",
      "22472 yeah\n",
      "18706 yes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the random forest...\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, train[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.\n",
      "498\n",
      " stellargirl I loooooooovvvvvveee my Kindle   Not that the DX is cool  but the   is fantastic in its own right \n",
      "498\n",
      "['stellargirl', 'i', 'loooooooovvvvvveee', 'my', 'kindle', 'not', 'that', 'the', 'dx', 'is', 'cool', 'but', 'the', 'is', 'fantastic', 'in', 'its', 'own', 'right']\n",
      "498\n",
      "498\n",
      "stellargirl loooooooovvvvvveee kindle dx cool fantastic right\n"
     ]
    }
   ],
   "source": [
    "test_tweet_text = [BeautifulSoup(raw_tweet,\"lxml\").get_text() for raw_tweet in test[5]]\n",
    "print(test_tweet_text[0])\n",
    "print(len(test_tweet_text))\n",
    "test_letters_only = [re.sub(\"[^a-zA-Z]\", \" \", tweet) for tweet in test_tweet_text]\n",
    "print(test_letters_only[0])\n",
    "print(len(test_letters_only))\n",
    "test_words = [tweets.lower().split() for tweets in test_letters_only]\n",
    "print(test_words[0])\n",
    "print(len(test_words))\n",
    "\n",
    "test_meaningful_words = [f(w) for w in test_words]\n",
    "\n",
    "test_bagofwords = []\n",
    "\n",
    "test_bagofwords = [list_to_string(w) for w in test_meaningful_words]\n",
    "print(len(test_bagofwords))\n",
    "print(test_bagofwords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(test_bagofwords)\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n",
      "[0 4 4 4 0 4 0 4 4 4 4 0 0 4 4 4 0 4 0 4 4 4 0 4 0 4 4 0 4 4 0 4 4 0 4 4 4\n",
      " 4 4 0 0 4 0 4 0 4 4 0 4 4 0 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4 4 4 4 0 0\n",
      " 4 4 4 0 4 0 4 0 4 4 4 0 4 0 4 4 4 4 0 0 0 0 4 0 0 0 4 4 4 4 4 0 4 4 4 4 4\n",
      " 4 4 4 4 0 4 4 4 4 4 4 4 0 4 4 4 4 4 4 0 4 4 0 4 4 0 4 4 4 4 4 0 0 4 0 0 4\n",
      " 0 0 4 4 4 0 4 4 4 0 4 4 4 4 0 4 0 4 4 0 4 4 0 4 4 4 4 4 0 4 4 4 4 4 4 4 0\n",
      " 4 4 4 4 0 4 0 4 4 4 4 0 4 4 4 4 4 4 4 4 0 0 4 4 4 0 0 4 0 0 4 4 0 0 4 0 4\n",
      " 4 4 0 0 4 4 0 4 4 0 4 4 0 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 0 4 4 4 0 4 0 4 4 4 0 0 4 4 4 4 0 0 4 0\n",
      " 0 4 4 4 4 4 4 4 4 4 0 0 4 4 4 4 0 4 4 4 0 4 4 4 0 4 4 4 4 4 4 4 4 4 4 4 0\n",
      " 0 4 4 0 4 4 4 0 4 4 4 0 4 4 4 4 4 4 4 4 0 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 0 4 4 4 4 0 4 0 0 4 4 4 4 4 0 4 4 4 0 4 4 4 4 4 4 4 0 4 4 0 0\n",
      " 4 0 0 4 4 4 4 4 4 4 0 4 4 0 4 0 4 4 4 4 0 0 0 4 4 4 4 4 4 4 4 0 4 0 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 0 4 4 0 0 4 4 4 4 4 0 4 4 4 4 0 4 4 0 0 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 0 4 4 0 0 4 0]\n"
     ]
    }
   ],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "print(len(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.441767068273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test[0],result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
